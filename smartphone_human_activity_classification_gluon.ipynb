{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gluon RNN Classifier for Human activity recognition\n",
    "\n",
    "Classify human activity based on smartphone accelerometer and gyroscope data with gluon. A handy extensible RNNClassifier Gluon is provided which makes it simple to use RNN for other problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Human Activity Recognition Using Smartphones Data Set \n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\n",
    "\n",
    "The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. \n",
    "\n",
    "The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Follow this link to see a video of how the data was collected:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"Video of the experiment\" width=\"400\" height=\"300\" border=\"10\" /></a>\n",
    "  <a href=\"https://youtu.be/XOEN9W05_4A\"><center>[Watch video]</center></a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# CODE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "You can download the dataset and unzip it with its existing folder structure from https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\n",
    "\n",
    "** To make it easier, its been provided in the data/ folder **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(7352, 128, 9)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "### Load Data\n",
    "train = [\"data/har_data/train/%strain.txt\" % signal for signal in INPUT_SIGNAL_TYPES]\n",
    "test = [\"data/har_data/test/%stest.txt\" % signal for signal in INPUT_SIGNAL_TYPES]\n",
    "\n",
    "def load_data(files):\n",
    "    arr = []\n",
    "    for fname in files:\n",
    "        with open(fname, 'r') as f:\n",
    "            rows = [row.replace('  ', ' ').strip().split(' ') for row in f]\n",
    "            arr.append([np.array(ele, dtype=np.float32) for ele in rows])\n",
    "    return np.transpose(np.array(arr), (1, 2, 0))\n",
    "\n",
    "X_train = load_data(train)\n",
    "X_test = load_data(test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((7352,), (2947,))"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_train_path = \"data/har_data/train/y_train.txt\"\n",
    "\n",
    "df = pd.read_csv(y_train_path, names=[\"labels\"])\n",
    "y_train = np.asarray(list(df.to_dict()['labels'].values())).astype('float32')\n",
    "\n",
    "y_test_path = \"data/har_data/test/y_test.txt\"\n",
    "df = pd.read_csv(y_test_path, names=[\"labels\"])\n",
    "y_test = np.asarray(list(df.to_dict()['labels'].values())).astype('float32')\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Gluon RNN Helper class has been provided here to help extend and reuse the same code for other problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# RNN Helper class\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "class BaseRNNClassifier(mx.gluon.Block):\n",
    "    '''\n",
    "    Exensible RNN class with LSTM that can operate with MXNet NDArray iter or DataLoader.\n",
    "    Includes fit() function to mimic the symbolic fit() function\n",
    "    '''\n",
    "    \n",
    "    @classmethod\n",
    "    def get_data(cls, batch, iter_type, ctx):\n",
    "        ''' get data and label from the iterator/dataloader '''\n",
    "        if iter_type == 'mxiter':\n",
    "            X = batch.data[0].as_in_context(ctx)\n",
    "            y = batch.label[0].as_in_context(ctx)\n",
    "        elif iter_type in [\"numpy\", \"dataloader\"]:\n",
    "            X = batch[0].as_in_context(ctx)\n",
    "            y = batch[1].as_in_context(ctx)\n",
    "        else:\n",
    "            raise ValueError(\"iter_type must be mxiter or numpy\")\n",
    "        return X, y\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all_labels(cls, data_iterator, iter_type):\n",
    "        if iter_type == 'mxiter':\n",
    "            pass\n",
    "        elif iter_type in [\"numpy\", \"dataloader\"]:\n",
    "            return data_iterator._dataset._label\n",
    "    \n",
    "    def __init__(self, ctx):\n",
    "        super(BaseRNNClassifier, self).__init__()\n",
    "        self.ctx = ctx\n",
    "\n",
    "    def build_model(self, n_out, rnn_size=128, n_layer=1):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        # LSTM default; #TODO(Sunil): make this generic\n",
    "        self.lstm = mx.gluon.rnn.LSTM(self.rnn_size, self.n_layer, layout='NTC')\n",
    "        #self.lstm = mx.gluon.rnn.GRU(self.rnn_size, self.n_layer)\n",
    "        self.output = mx.gluon.nn.Dense(self.n_out)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = out[:, out.shape[1]-1, :]\n",
    "        out = self.output(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def compile_model(self, loss=None, lr=3E-3):\n",
    "        self.collect_params().initialize(mx.init.Xavier(), ctx=self.ctx)\n",
    "        self.criterion = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "        self.loss = mx.gluon.loss.SoftmaxCrossEntropyLoss() if loss is None else loss\n",
    "        self.lr = lr\n",
    "        self.optimizer = mx.gluon.Trainer(self.collect_params(), 'adam', \n",
    "                                          {'learning_rate': self.lr})\n",
    "\n",
    "    def top_k_acc(self, data_iterator, iter_type='mxiter', top_k=3, batch_size=128):\n",
    "        batch_pred_list = []\n",
    "        true_labels = []\n",
    "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "        hidden = [init_state] * 2\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "            batch_pred = self.forward(data, hidden)\n",
    "            #batch_pred = mx.nd.argmax(batch_pred, axis=1)\n",
    "            batch_pred_list.append(batch_pred.asnumpy())\n",
    "            true_labels.append(label)\n",
    "        y = np.vstack(batch_pred_list)\n",
    "        true_labels = np.vstack(true_labels)\n",
    "        argsorted_y = np.argsort(y)[:,-top_k:]\n",
    "        return np.asarray(np.any(argsorted_y.T == true_labels, axis=0).mean(dtype='f'))\n",
    "    \n",
    "    def evaluate_accuracy(self, data_iterator, metric='acc', iter_type='mxiter', batch_size=128):\n",
    "        met = mx.metric.Accuracy()\n",
    "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "        hidden = [init_state] * 2\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "            # Lets do a forward pass only!\n",
    "            output, hidden = self.forward(data, hidden)\n",
    "            preds = mx.nd.argmax(output, axis=1)\n",
    "            met.update(labels=label, preds=preds)\n",
    "                \n",
    "        #if self.all_labels is None:\n",
    "        #    self.all_labels = BaseRNNClassifier.get_all_labels(data_iterator, iter_type)\n",
    "        #preds = self.predict(data_iterator, iter_type=iter_type, batch_size=batch_size)\n",
    "        #met.update(labels=mx.nd.array(self.all_labels[:len(preds)]), preds=preds)\n",
    "        \n",
    "        return met.get()                   \n",
    "                    \n",
    "    def predict(self, data_iterator, iter_type='mxiter', batch_size=128):\n",
    "        batch_pred_list = []\n",
    "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "        hidden = [init_state] * 2\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "            output, hidden = self.forward(data, hidden)\n",
    "            batch_pred_list.append(output.asnumpy())\n",
    "        #return np.vstack(batch_pred_list)\n",
    "        return np.argmax(np.vstack(batch_pred_list), 1)\n",
    "    \n",
    "    def fit(self, train_data, test_data, epochs, batch_size, verbose=True):\n",
    "        '''\n",
    "        @train_data:  can be of type list of Numpy array, DataLoader, MXNet NDArray Iter\n",
    "        '''\n",
    "        \n",
    "        moving_loss = 0.\n",
    "        total_batches = 0\n",
    "\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        test_acc = []\n",
    "\n",
    "        iter_type = 'numpy'\n",
    "        train_iter = None\n",
    "        test_iter = None\n",
    "        print(\"Data type:\", type(train_data), type(test_data), iter_type, type(train_data[0]))\n",
    "        \n",
    "        # Can take MX NDArrayIter, or DataLoader\n",
    "        if isinstance(train_data, mx.io.NDArrayIter):\n",
    "            train_iter = train_data\n",
    "            test_iter = test_data\n",
    "            iter_type = 'mxiter'\n",
    "            #total_batches = train_iter.num_data // train_iter.batch_size\n",
    "\n",
    "        elif isinstance(train_data, list):\n",
    "            if isinstance(train_data[0], np.ndarray) and isinstance(train_data[1], np.ndarray):\n",
    "                X, y = np.asarray(train_data[0]).astype('float32'), np.asarray(train_data[1]).astype('float32')\n",
    "                tX, ty = np.asarray(test_data[0]).astype('float32'), np.asarray(test_data[1]).astype('float32')\n",
    "                \n",
    "                total_batches = X.shape[0] // batch_size\n",
    "                train_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X, y), \n",
    "                                    batch_size=batch_size, shuffle=True, last_batch='discard')\n",
    "                test_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(tX, ty), \n",
    "                                    batch_size=batch_size, shuffle=False, last_batch='discard')\n",
    "                \n",
    "        elif isinstance(train_data, mx.gluon.data.dataloader.DataLoader) and isinstance(test_data, mx.gluon.data.dataloader.DataLoader):\n",
    "            train_iter = train_data\n",
    "            test_iter = test_data\n",
    "            total_batches = len(train_iter)\n",
    "        else:\n",
    "            raise ValueError(\"pass mxnet ndarray or numpy array as [data, label]\")\n",
    "\n",
    "        print(\"Data type:\", type(train_data), type(test_data), iter_type)\n",
    "        print(\"Sizes\", self.n_layer, batch_size, self.rnn_size, self.ctx)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            #print self.lstm.collect_params()\n",
    "\n",
    "            # reset iterators if of MXNet Itertype\n",
    "            if iter_type == \"mxiter\":\n",
    "                train_iter.reset()\n",
    "                test_iter.reset()\n",
    "        \n",
    "            init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "            hidden = [init_state] * 2                \n",
    "            #hidden = self.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=self.ctx)\n",
    "            yhat = []\n",
    "            for i, batch in enumerate(train_iter):\n",
    "                data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "                #print \"Data Shapes:\", data.shape, label.shape\n",
    "                hidden = detach(hidden)\n",
    "                with mx.autograd.record(train_mode=True):\n",
    "                    preds, hidden = self.forward(data, hidden)\n",
    "                    #print preds[0].shape, hidden[0].shape, label.shape\n",
    "                    loss = self.loss(preds, label) \n",
    "                    yhat.extend(preds)\n",
    "                loss.backward()                                        \n",
    "                self.optimizer.step(batch_size)\n",
    "                preds = mx.nd.argmax(preds, axis=1)\n",
    "                \n",
    "                batch_acc = mx.nd.mean(preds == label).asscalar()\n",
    "\n",
    "                if i == 0:\n",
    "                    moving_loss = nd.mean(loss).asscalar()\n",
    "                else:\n",
    "                    moving_loss = .99 * moving_loss + .01 * mx.nd.mean(loss).asscalar()\n",
    "                    \n",
    "                if verbose and i%100 == 0:\n",
    "                    print('[Epoch {}] [Batch {}/{}] Loss: {:.5f}, Batch acc: {:.5f}'.format(\n",
    "                          e, i, total_batches, moving_loss, batch_acc))                    \n",
    "                    \n",
    "            train_loss.append(moving_loss)\n",
    "            \n",
    "            t_acc = self.evaluate_accuracy(train_iter, iter_type=iter_type, batch_size=batch_size)\n",
    "            train_acc.append(t_acc[1])\n",
    "            \n",
    "            tst_acc = self.evaluate_accuracy(test_iter, iter_type=iter_type, batch_size=batch_size)\n",
    "            test_acc.append(tst_acc[1])\n",
    "\n",
    "            print(\"Epoch %s. Loss: %.5f Train Acc: %s Test Acc: %s\" % (e, moving_loss, t_acc, tst_acc))\n",
    "        return train_loss, train_acc, test_acc\n",
    "                    \n",
    "    def predict(self, data_iterator, iter_type='mxiter', batch_size=128):\n",
    "        batch_pred_list = []\n",
    "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\n",
    "        hidden = [init_state] * 2\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\n",
    "            output, hidden = self.forward(data, hidden)\n",
    "            batch_pred_list.append(output.asnumpy())\n",
    "        return np.argmax(np.vstack(batch_pred_list), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Data type: <class 'list'> <class 'list'> numpy <class 'numpy.ndarray'>\n",
      "Data type: <class 'list'> <class 'list'> numpy\nSizes 1 32 64 cpu(0)\n",
      "[Epoch 0] [Batch 0/229] Loss: 1.78948, Batch acc: 0.21875\n",
      "[Epoch 0] [Batch 100/229] Loss: 1.40709, Batch acc: 0.28125\n",
      "[Epoch 0] [Batch 200/229] Loss: 1.14173, Batch acc: 0.56250\n",
      "Epoch 0. Loss: 1.06675 Train Acc: ('accuracy', 0.47161572052401746) Test Acc: ('accuracy', 0.45754076086956524)\n[Epoch 1] [Batch 0/229] Loss: 1.01739, Batch acc: 0.31250\n",
      "[Epoch 1] [Batch 100/229] Loss: 0.80033, Batch acc: 0.59375\n",
      "[Epoch 1] [Batch 200/229] Loss: 0.65030, Batch acc: 0.53125\n",
      "Epoch 1. Loss: 0.64365 Train Acc: ('accuracy', 0.5604530567685589) Test Acc: ('accuracy', 0.5360054347826086)\n[Epoch 2] [Batch 0/229] Loss: 0.89742, Batch acc: 0.46875\n",
      "[Epoch 2] [Batch 100/229] Loss: 0.69525, Batch acc: 0.62500\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "ctx = mx.cpu() # .gpu(0) #change context to execute on CPU\n",
    "model = BaseRNNClassifier(ctx)\n",
    "model.build_model(n_out=len(LABELS), rnn_size=64, n_layer=1)\n",
    "model.compile_model()\n",
    "train_loss, train_acc, test_acc = model.fit([X_train, y_train], [X_test, y_test], batch_size=32, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets plot accuracy and loss over each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss, label=\"loss\")\n",
    "plt.plot(train_acc, label=\"train\")\n",
    "plt.plot(test_acc, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Loss plot\n",
    "\n",
    "plt.plot(train_loss, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise \n",
    "\n",
    "1. Build the confusion matrix \n",
    "2. Try plotting the misclassified samples\n",
    "3. Can we use a CNN to solve this problem? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-f4521f13",
   "language": "python",
   "display_name": "PyCharm (timeseries-autoencoder-lstm)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}